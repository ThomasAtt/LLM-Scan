LLM-Scan是一款用于大模型攻击测试的工具，模拟提示词注入等场景用以测试大模型的安全性。

工具基于微软的llm攻击框架PyRIT框架实现

项目地址：https://github.com/Azure/PyRIT

具体实现就是将PyRIT中的部分模块图形化

目前实现了针对大模型的多轮攻击的以及对每轮的回答结果进行打分，先调用其他大模型针对要问的恶意问题生成提示词，然后再调用生成的提示词对需要测试的目标模型进行攻击，最后调用评分模型对每轮回答的结果的危害性进行评分。

![image-20250928164306349](./流程图.png)

攻击和打分器都是调用的PyRIT中的模块

攻击页面如下：

![img](./主页.png)

为保证效果，攻击模型建议采用性能较好，且安全设置不严格的模型。

打分模型和攻击模型是复用的，每次攻击可以设置**温度、token、攻击轮次**等参数，用以测试私有化部署的模型在不同温度下的安全表现。

**测试用的攻击模型是心流的中部署的千问3，Api可以在这儿申请：**

[**https://platform.iflow.cn/docs/**](https://platform.iflow.cn/docs/)

攻击目标语句可以自己设置，这个可以放恶意测试集，可以参考《TC260生成式人工智能服务安全基本要求》，目标模型可以是私有化部署的模型，也可以是网络上其他支持OpenAi调用的模型。**攻击模型性能越好测试的效果越好，可能会出现输出的内容不完整的情况，需要将目标模型的最大令牌数设置大一些。**

每次攻击完成后可以对目标模型输出的内容进行打分，打的分值在0-1之间，分数越高危害越大。

![img](./结果.png)

可将结果保存到本地

![img](./保存.png)